{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "os.chdir(\"/mnt/arrakis/sietch/VectaLearn\")\n",
    "sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from server.vecta_learn import patch\n",
    "\n",
    "patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, here's one for you: \n",
      "\n",
      "So, I was smoking some of that good ol' Devil's Lettuce the other day, and I had this epiphany, man. You ever notice how mosquitoes are like the vegans of the insect world? They suck on you for their precious blood, but the second you give them a taste of their own medicine and swat 'em away, they're all like, \"Dude, that's not cool, man.\" I mean, talk about hypocrisy, right? I'm all for peace and love, but if you're gonna snack on my arm, you better be ready for the consequences, bro."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "res = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are JokeBot. An AI that tells jokes in the style of Joe Rogan.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a joke.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for chunk in res:\n",
    "    content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n",
    "    if content is not None:\n",
    "        print(content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f7eef174730>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich.traceback import install\n",
    "install(show_locals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/server/rag/parsing.py\n",
    "#\n",
    "# Code modified from LlamaIndex by Jerry Liu\n",
    "\n",
    "from typing import Any, List, Optional, Sequence, Union\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from llama_index.schema import BaseNode, Document, IndexNode, TextNode\n",
    "from llama_index.node_parser.interface import NodeParser\n",
    "from llama_index.callbacks.base import CallbackManager\n",
    "from llama_index.llms.openai import LLM\n",
    "\n",
    "\n",
    "TABLE_SUMMARY_QUERY = \"\"\"\\\n",
    "What is this table about? Give a very concise summary (imagine you are adding a \\\n",
    "caption), and also output whether or not the table should be kept. \\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class UnstructuredParser(NodeParser):\n",
    "    \"\"\"Unstructured element node parser\n",
    "    \n",
    "    Splits a document into TextNodes and Index Nodes corresponding to embedded\n",
    "    objects (e.g. tables, images).\n",
    "    \"\"\"\n",
    "    \n",
    "    callback_manager: CallbackManager = Field(\n",
    "        default_factory=CallbackManager, exclude=True\n",
    "    )\n",
    "    llm: Optional[LLM] = Field(\n",
    "        default=None, description=\"LLM model to use for summarization.\"\n",
    "    )\n",
    "    table_summary_query_str: str = Field(\n",
    "        default=TABLE_SUMMARY_QUERY,\n",
    "        description=\"Query string to use for summarization.\",\n",
    "    )\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        llm: Optional[Any] = None,\n",
    "        table_summary_query_str: str = TABLE_SUMMARY_QUERY,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize UnstructuredParser\"\"\"\n",
    "\n",
    "        try:\n",
    "            import lxml\n",
    "            import unstructured\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"This node parser requires the `lxml` and `unstructured` packages.\"\n",
    "            )\n",
    "        \n",
    "        callback_manager = callback_manager or CallbackManager([])\n",
    "        \n",
    "        return super().__init__(\n",
    "            callback_manager=callback_manager,\n",
    "            llm=llm,\n",
    "            table_summary_query_str=table_summary_query_str,\n",
    "        )\n",
    "    \n",
    "    def get_nodes_from_documents(\n",
    "        self,\n",
    "        documents: Sequence[str],\n",
    "        show_progress: bool = False,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Parse document(s) into nodes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        documents : Sequence[str]\n",
    "            Sequence of documents to parse.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.flat_reader import FlatReader\n",
    "from pathlib import Path\n",
    "from rich import inspect\n",
    "\n",
    "reader = FlatReader()\n",
    "docs_2021 = reader.load_data(Path(\"data/tesla_2021_10k.htm\"))\n",
    "docs_2020 = reader.load_data(Path(\"data/tesla_2020_10k.htm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch openai\n",
    "import openai\n",
    "import llama_index.llms.openai_utils\n",
    "import os\n",
    "\n",
    "def patched_create(original_create):\n",
    "    def new_create(*args, **kwargs):\n",
    "        headers = kwargs.get('headers', {})\n",
    "        headers[\"Helicone-Auth\"] = f\"Bearer {os.environ['HELICONE_API_KEY']}\"\n",
    "        kwargs['headers'] = headers\n",
    "        return original_create(*args, **kwargs)\n",
    "    return new_create\n",
    "\n",
    "# Backup original methods\n",
    "original_completion_create = openai.Completion.create\n",
    "original_chat_completion_create = openai.ChatCompletion.create\n",
    "\n",
    "# Patch the methods\n",
    "openai.Completion.create = patched_create(original_completion_create)\n",
    "openai.ChatCompletion.create = patched_create(original_chat_completion_create)\n",
    "\n",
    "# Now when you use llama_index.llms.openai_utils.get_completion_endpoint,\n",
    "# the headers argument will be included automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create UnstructuredElementNodeParser from LLamaIndex\n",
    "from llama_index.node_parser import UnstructuredElementNodeParser\n",
    "from rag.llm import SmithOpenAI\n",
    "\n",
    "node_parser = UnstructuredElementNodeParser(llm=SmithOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists(\"data/2021_nodes.pkl\"):\n",
    "    raw_nodes_2021 = node_parser.get_nodes_from_documents(docs_2021)\n",
    "    pickle.dump(raw_nodes_2021, open(\"data/2021_nodes.pkl\", \"wb\"))\n",
    "else:\n",
    "    raw_nodes_2021 = pickle.load(open(\"data/2021_nodes.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(raw_nodes_2021)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
